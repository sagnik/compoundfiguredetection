{"Caption":"Figure 1: A human example path between the concepts DIK - DIK and ALBERT EINSTEIN. Nodes represent Wikipedia arti- cles and edges the hyperlinks clicked by the human. Edge labels indicate the order of clicks, the framed numbers the shortest- path length to the target. One of several optimal solutions would be DIK - DIK , WATER, GERMANY, ALBERT EINSTEIN . ","ImageText":[{"Text":"Information","TextBB":[426.103,99.6352,562.592,123.371],"Rotation":0},{"Text":"Networks","TextBB":[569.374,99.6352,680.833,123.371],"Rotation":0},{"Text":"Jure","TextBB":[527.499,162.066,559.421,180.065],"Rotation":0},{"Text":"Leskovec","TextBB":[564.028,162.066,633.128,180.065],"Rotation":0},{"Text":"Computer","TextBB":[485,178.594,545.541,193.593],"Rotation":0},{"Text":"Science","TextBB":[549.594,178.594,598.859,193.593],"Rotation":0},{"Text":"Department","TextBB":[602.662,178.594,675.308,193.593],"Rotation":0},{"Text":"Stanford","TextBB":[522.332,193.095,574.163,208.094],"Rotation":0},{"Text":"University","TextBB":[577.973,193.095,638.421,208.094],"Rotation":0},{"Text":"jure@cs.stanford.edu","TextBB":[501.667,207.562,658.723,225.562],"Rotation":0},{"Text":"2","TextBB":[550.433,307.586,555.433,318.656],"Rotation":0},{"Text":"3","TextBB":[472.528,344.21,477.528,355.28],"Rotation":0},{"Text":"1","TextBB":[516.028,344.53,521.028,355.6],"Rotation":0},{"Text":"-","TextBB":[588.964,292.55,600.064,329.45],"Rotation":0},{"Text":"e","TextBB":[566.425,298.602,586.158,347.802],"Rotation":0},{"Text":"2","TextBB":[696.782,303.221,701.782,314.291],"Rotation":0},{"Text":"3","TextBB":[623.798,305.49,628.798,316.56],"Rotation":0},{"Text":"4","TextBB":[623.958,333.66,628.958,344.73],"Rotation":0},{"Text":"ALBERT","TextBB":[708.267,347.137,738.067,355.747],"Rotation":0},{"Text":"EINSTEIN","TextBB":[705.653,354.915,740.678,363.525],"Rotation":0},{"Text":"5","TextBB":[609.278,347.55,614.278,358.62],"Rotation":0},{"Text":"2","TextBB":[564.678,362.23,569.678,373.3],"Rotation":0},{"Text":"6","TextBB":[670.778,380.48,675.778,391.55],"Rotation":0},{"Text":"2","TextBB":[505.83,399.014,510.83,410.084],"Rotation":0},{"Text":"1","TextBB":[594.56,400.999,599.56,412.069],"Rotation":0},{"Text":"WATER","TextBB":[525.629,416.274,551.544,424.884],"Rotation":0},{"Text":"0","TextBB":[692.099,403.935,697.099,415.005],"Rotation":0},{"Text":"QUANTUM","TextBB":[608.999,417.378,648.767,425.988],"Rotation":0},{"Text":"MECHANICS","TextBB":[605.958,425.156,651.798,433.766],"Rotation":0},{"Text":"ATOM","TextBB":[655.234,289.13,677.144,297.74],"Rotation":0},{"Text":"ELECTRON","TextBB":[564.44,290.795,605.088,299.405],"Rotation":0},{"Text":"DIK-DIK","TextBB":[481.187,291.114,511.452,299.724],"Rotation":0}],"Mention":["to the target thereafter. While approaching the target through a se-\nries of conceptually very related articles is safer and often humans\u2019\npreferred solution (cf. the example of Fig. 1), it is typically not the\nmost efficient: we find that thinking \u2018out of the box\u2019 often allows\ninformation seekers to find shorter paths between concepts\u2014at the\nrisk of getting lost. A strategy that is both popular and often suc-\ncessful is to connect concepts in terms of their geographical com-\nmonalities. In the above example, ã€ˆDIK-DIK, AFRICA, EUROPE,\n","Present work. These broad issues suggest a wide range of inter-\nesting open questions. We take a step in this direction by compu-\ntationally analyzing how people navigate to specific target pages in\nthe Wikipedia information network. As a tool we use the online\nhuman-computation game Wikispeedia [24, 23], in which players\n(i.e., information seekers) are given two random articles and aim\nto solve the task of navigating from one to the other by clicking\nas few hyperlinks as possible. Players have no knowledge of the\nglobal network structure but must rely solely on the local informa-\ntion they see on each page\u2014the outgoing links connecting the cur-\nrent article to its neighbors\u2014and on their expectations about which\narticles are likely to be interlinked. In this respect, the task humans\nare trying to solve at each visited article is that of guessing which of\nthe outgoing links to follow in order to eventually reach the target\narticle.\nWhat makes our study unique is that we have been collecting de-\ntailed data on more than 30,000 instances of human wayfinding in\nan information network describing general human knowledge (the\ndata came from around 9,400 distinct IP addresses). This allows\nus to computationally analyze human wayfinding on a large scale.\nEven more important, for every instance we know the starting arti-\ncle and the given target article the user is trying to reach. Hence, we\ndo not have to infer or guess the information need of the informa-\ntion seeker, but can base our methods on the ground truth instead.\nTo illustrate the dynamics of the Wikispeedia game, as well as\npotential reasoning schemes and classes of strategies humans might\nuse, Fig. 1 gives the example of a human path between the start ar-\nticle DIK-DIK and the target ALBERT EINSTEIN. (We call such a\n","harder but also more useful than the first; e.g., if the method is to\nbe implemented for an intelligent browsing tool, reasonable targets\nmust be picked from all candidates, not just from a set of two.\nIn the first task, we use accuracy as a metric; in the second, we\nmeasure ranking loss according to cumulative reciprocal rank, the\nobjective we also use for training. While this captures ranking qual-\nity objectively, it might be overly strict; e.g., if t = WINE, then\npredicting BEER is much better than, say, GASOLINE. We account\nfor this by measuring \u2018sibling precision@m\u2019, which is the same as\nprecision@m, with the difference that not only t but all articles from\nthe same category as t are counted as relevant (we use the leaves of\nthe hierarchy of our Wikipedia version as categories).\nWe vary two parameters of the test prefixes: k, the number of\narticles in the prefix; and n, the length of the entire human path. The\nresults are summarized in Fig. 10 and 11. In all plots, the bold solid\n","line represents the multinomial ranking (MR) model, the thin solid\nline the binomial logistic regression (BLR) model, and the dashed\nline the TF-IDF baseline. First note that MR is at least as good\nas, and often better than, both BLR and TF-IDF according to every\nmetric. Now consider Fig. 10. As expected, our methods work\nbetter when prefixes are longer (cf. the order of the bold curves) and\nwhen full paths are shorter (cf. the slopes of the curves). Notably,\non the task of picking the correct one of two targets, MR achieves\nan accuracy of 80% when 3 clicks are seen, regardless of whether\nthe entire game is 4, 5, or 6 clicks long. Interestingly, while BLR\nhas higher accuracy on the binary task, the simple TF-IDF baseline\nachieves better ranking performance. We take this as an indicator\nthat MR combines the better properties of both.\nFinally, consider Fig. 11, which shows sibling precision@m. For\nthe sake of brevity, we display only the case k = 4, but in relative\nterms the results are the same for all prefix lengths. The preci-\nsion@30 of MR is 20% for n= 5, which means that 6 of the top 30\ntargets are of the same category as the true target, when we see 3\nclicks and the full game has 1 more click. Even when there are 2 (3)\nmore clicks, we still see 5 (3) top-ranked articles that are very close\nto the true target (for comparison, in a random ranking, precision\nis only 1% on average). This property of the ranking algorithm is\ndesirable, since in a real-world application making a close enough\nguess might often be nearly as good as predicting the exact target.\n"],"Page":1,"Number":1,"Type":"Figure","CaptionBB":[438,445,772,531],"Height":1100,"Width":850,"DPI":100,"ImageBB":[425,96,753,435]}