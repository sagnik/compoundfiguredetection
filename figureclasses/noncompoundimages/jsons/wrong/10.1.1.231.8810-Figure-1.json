{"Caption":"Figure 1: Components of a CA-SSD compared to traditional SSD. CA-SSD has two new hardware elements: (i) a hashing co-processor and (ii) a battery-backed RAM (BB-RAM). Furthermore, CA-SSD stores hashes instead of LPN in the page OOB area. Also shown is a comparison of how writes are handled in the two devices. (a) Traditional SSD: (1-2) On receiving a write request from device driver, SSD controller issues a flash page write. (3-4) On completion, the Mapping Table in the volatile RAM is updated and driver is notified of request completion. (b) CA-SSD: (1-2) On receiving a write request, the SSD controller sends the content to the hash co-processor for hash computation. (3-4) The returned hash is then looked up in the Mapping Table in the BB-RAM. (5-6(a)) On a hit, the mapping structures are updated and the request completes. (5-9(b)) On a miss, a flash page write is performed, mapping structures are updated and the request is completed. ","ImageText":[],"Mention":["In Figure 1(b), we present the additional compo-\nnents\/functionality (compared to a traditional drive) re-\nquired by CA-SSD. For both devices, we also show the\nsteps involved in processing requests coming from the\nblock device driver to help understand the difference in\ntheir operation. We refer to the FTL in CA-SSD as CA-\nFTL. Read requests are handled identically in both the\nSSDs and so we only focus on write requests. Whereas a\ntraditional SSD requires all writes to be sent to physical\npages, CA-SSD returns a write request without requiring\nflash page writes if hashes, representing their content, are\nfound in RAM. We require four key enhancements to a\ntraditional SSD to achieve this functionality.\n","(iii) Persistent Meta-data Store: Our CA-SSD de-\nsign necessitates a re-consideration of the mechanism\nfor recovering the contents of the meta-data cache after\na power failure. When writing a physical page (PPN),\na traditional FTL also stores the logical page number\n(LPN) in a special-purpose part of the PPN called the\nout-of-band (OOB) area, which is typically 64-224 B in\nsize. After a power failure, these entries in the OOB\nare used to reconstruct the LPN-to-PPN mappings. In\nCA-FTL, multiple LPNs may contain the same value and\nhence correspond to the same PPN. The OOB area may\nnot have enough room for all these LPNs. Furthermore, a\nvalue can be associated with a changing set of LPNs over\nits lifetime, requiring multiple writes to the same OOB\narea, with corresponding erase\/copying operations. We\naddress this difficulty by requiring that CA-FTL\u2019s Map-\nping Table be kept in a fast persistent storage in the first\nplace, without any need to store a copy on flash. Storing\na copy on flash would result in large number of meta-\ndata writes on flash increasing the number of flash page\nwrites. An alternative approach could be to perform peri-\nodic check-pointing of Mapping Table instead of imme-\ndiate writes on flash to reduce the number of meta-data\nwrites, thereby providing weaker guarantees on meta-\ndata consistency. In order to provide consistency guaran-\ntees similar to existing SSDs without impacting the over-\nall performance, we employ persistent battery-backed\nRAM. We indicate this as BB-RAM in Figure 1(b).\nWrite caches based on such battery-backed DRAM are\ncommonly used in RAID controllers [3]. Even SSDman-\nufacturers have started providing battery-backed DRAM\nas a standard feature to deal with power failures [5, 4].\n"],"Page":4,"Number":1,"Type":"Figure","CaptionBB":[98,278,749,432],"Height":1100,"Width":850,"DPI":100,"ImageBB":[114,98,736,267]}