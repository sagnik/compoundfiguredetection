{"Caption":"Figure 1: Graphical models showing dependencies among variables in (a) the simple two- stage model, and (b) the morphology model. Shading of the node containing w reflects the fact that this variable is observed. Dotted lines delimit the generator and adaptor. ","ImageText":[{"Text":"(a)","TextBB":[174.333,99.4944,188.147,110.702],"Rotation":0},{"Text":"Generator","TextBB":[216.333,99.4944,265.843,110.702],"Rotation":0},{"Text":"Adaptor","TextBB":[310.558,99.4944,351.205,110.702],"Rotation":0},{"Text":"(b)","TextBB":[393.998,99.4944,408.478,110.702],"Rotation":0},{"Text":"Generator","TextBB":[476.332,99.4944,525.842,110.702],"Rotation":0},{"Text":"Adaptor","TextBB":[610.832,99.4944,651.479,110.702],"Rotation":0},{"Text":"θ","TextBB":[237.167,136.31,244.739,148.166],"Rotation":0},{"Text":"z","TextBB":[327.001,136.377,335.303,148.1],"Rotation":0},{"Text":"c","TextBB":[452.167,136.377,460.469,148.1],"Rotation":0},{"Text":"t","TextBB":[537.668,136.377,544.94,148.1],"Rotation":0},{"Text":"z","TextBB":[627.175,136.377,635.478,148.1],"Rotation":0},{"Text":"ℓ","TextBB":[237.334,221.344,245.204,233.199],"Rotation":0},{"Text":"w","TextBB":[324.334,221.377,337.834,233.1],"Rotation":0},{"Text":"f","TextBB":[452.5,221.377,458.212,233.1],"Rotation":0},{"Text":"ℓ","TextBB":[537.334,221.344,545.204,233.199],"Rotation":0},{"Text":"w","TextBB":[624.338,221.377,637.837,233.1],"Rotation":0}],"Mention":["second stage is to generate the actual sequence of words itself. This is done by allowing a\nsequence of customers to enter the restaurant. Each customer chooses a table, producing a\nseating arrangement, z, and says the word used to label that the table, producing a sequence\nof words, w. The process by which customers choose tables, which we will refer to as the\nadaptor, defines a probability distribution over the sequence of words w produced by the\ncustomers, determining the frequency with which tokens of the different types occur. The\nstatistical dependencies among the variables in one such model are shown in Figure 1 (a).\n","then drawing a stem and a suffix conditioned on the class. Each of these draws is from a\nmultinomial distribution, and we will assume that these multinomials are in turn generated\nfrom symmetric Dirichlet priors, with parameters κ, τ , and φ respectively. The resulting\ngenerative model can be used as the generator in a two-stage language model, providing a\nmore structured replacement for the multinomial distribution, θ. As before, we will use the\nPitman-Yor process as an adaptor, setting b = 0. Figure 1 (b) illustrates the dependencies\nbetween the variables in this model.\n"],"Page":3,"Number":1,"Type":"Figure","CaptionBB":[173,271,675,316],"Height":1100,"Width":850,"DPI":100,"ImageBB":[172,97,674,256]}