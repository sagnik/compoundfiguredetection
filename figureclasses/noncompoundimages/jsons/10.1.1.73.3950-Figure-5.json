{"Caption":"                                                              ❢❣                ❤✌✐❥❯❧❦✣♠❵♥❜❳ Figure 5: Fitting a gamma Distribution through , to the y-position: we demonstrate our                                                  ❦     ♥ approach by fitting a gamma distribution, through the latent variables ♦ ✐q♣ rts✉♠❵r❜✈✉♠❵r✭♠✱✇\u0080①③② tion of the patches. Here we allowed and to be a 3rd degree polynomial function of y (i.e.                                                                                             ❪                                                                                               , to the y posi-     ④                                 ⑤⑦⑥                             ). The center-left square shows, on each row, a distribution of conditioned on the position of the left patch ( ) for each bi-patch, for training data taken from matching vehicles. The center-right square shows the same distributions for mismatched data. The height of histogram nomial fit to the conditional means, while the outer curves show the          ⑧⑨♥ distributions is color-coded, dark red indicating higher density. The central curve shows the poly-                                                                                    range. For reference, we include a partial image of a car whose y-coordinate is aligned with the center images. On the right, we show two histogram plots, each corresponding to one row of the center images (a small range of y corresponding to the black arrows). The resulting gamma distributions are superimposed on the histograms, and seem to fit the data well. ","ImageText":[],"Mention":["parameters \u0082\n\u0001\n9\nf\n\u0011\ng\n< 6 (Figure 5). This gives each conditional distribution higher bias but\nlower variance and is appropriate for sparse data conditions.\n","nomial function (i.e. a linear combination of powers) of the conditioning features.8 This\ncoupling of distributions serves to control the capacity of our model and exploit relation-\nships between the conditional distributions of\nI\nto better estimate conditional distributions\nfrom sparse data. These ideas are illustrated in Figure 5. The curves are estimated by max-\nimizing the likelihood of the values of\nI\nin the training set. Note that the linear weights \u0083 \u0084\n"],"Page":5,"Number":5,"Type":"Figure","CaptionBB":[173,147,675,335],"Height":1100,"Width":850,"DPI":100,"ImageBB":[173,29,678,148]}