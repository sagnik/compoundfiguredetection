{"Caption":"Figure 5: Screenshot showing spectrograms for the cor- rupted query and the matching region from the correctly- identified song in the database. ","ImageText":[],"Mention":["The music retrieval application consists of two parts \u2014 the\nmusic identification server and the graphical user interface\n(GUI), which can run on different machines and commu-\nnicate over sockets. We first describe how we generate the\naudio signature database, then describe the query phase, and\nfinally touch on the user interface.\nFor each song in the database, we build an audio sig-\nnature as described below; the same preprocessing is later\nused on the query snippet. We first compute the spectro-\ngram image, as in [8]. We convert each song into mono\nand downsample to 5512.5 KHz. For a CD quality audio\nsignal sampled at 44.1 KHz, we convolve the signal with\na low pass filter and take every 8th sample. Next, we ap-\nply a short-term Fourier transform with a window size of\n2048 samples (0.372 s) with successive windows offset by\n64 samples (11.6 ms). We divide the power between 300 Hz\nand 2000 Hz into 33 logarithmically spaced bands. This\nfrequency range corresponds to the range that can be easily\ntransmitted over mobile phones. We use logarithmic spac-\ning since the power distribution of typical audio is approxi-\nmately logarithmic across frequency. Finally, we apply the\n32 learned filters and thresholds to get a 32-bit descrip-\ntor for every time step (11.6 ms) of the signal; this series\nof descriptors is known as the signature. For an average-\nlength song of 200 seconds, the storage requirement for this\nrepresentation is approximately 70KB. We load all of the\ndescriptors into memory and put them into a hash table,\nalong with the song id and frame id (temporal offset). Al-\nthough the main memory implementation works well for a\nfew thousand songs, other methods may be necessary for\nlarger databases.\nDuring the query phase, the music identification server\nbuilds a similar set of 32-bit descriptors for the audio snip-\npet. For each descriptor, it finds all descriptors that are\nwithin a Hamming distance of 2 bits (empirically deter-\nmined to be a good balance between accuracy and match-\ning speed). Finally, we perform geometric alignment us-\ning RANSAC and find the best match according to the EM\nscore (Equation 3).\nThe GUI frontend records audio clips from the micro-\nphone and sends the waveform to the server for identifica-\ntion. Figure 5 shows a screenshot of our GUI. The bot-\ntom left and right panels displays the spectrogram of the\nrecorded and original songs, respectively. Although the raw\nspectrograms look different due to noise and occlusion, one\ncan see similar structures. The text panel gives the name of\nthe the song, correctly identified.\n"],"Page":6,"Number":5,"Type":"Figure","CaptionBB":[439,251,769,295],"Height":1100,"Width":850,"DPI":100,"ImageBB":[450,98,755,249]}