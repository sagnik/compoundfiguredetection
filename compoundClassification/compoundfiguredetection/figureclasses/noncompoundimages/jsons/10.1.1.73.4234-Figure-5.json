{"Caption":"Figure 5: Architecture of convolutional network used for training. This represents one slice of the","ImageText":[],"Mention":["The network architecture used for training is shown in Figure 5. It is similar to LeNet5 (LeCun\net al., 1998), but contains more feature maps. The network input is a 32\u0002 32 pixel gray-scale\nimage. The first layer C1 is a convolutional layer with 8 feature maps of size 28\u0002 28. Each unit\nin each feature map is connected to a 5\u0002 5 neighborhood in the input. Contiguous units in C1\ntake input from neighborhood on the input that overlap by 4 pixels. The next layer, S2, is a so-\ncalled subsampling layer with 8 feature maps of size 14\u0002 14. Each unit in each map is connected\nto a 2\u0002 2 neighborhood in the corresponding feature map in C1. Contiguous units in S2 take\ninput from contiguous, non-overlapping 2x2 neighborhoods in the corresponding map in C1. C3 is\nconvolutional with 20 feature maps of size 10\u0002 10. Each unit in each feature map is connected to\nseveral 5\u0002 5 neighborhoods at identical locations in a subset of S2\u2019s feature maps. Different C3\nmaps take input from different subsets of S2 to break the symmetry and to force the maps to extract\ndifferent features. S4 is a subsampling layer with 2\u0002 2 subsampling ratios containing 20 feature\nmaps of size 5\u00025. Layer C5 is a convolutional layer with 120 feature maps of size 1\u00021 with 5\u00025\nkernels. Each C5 map takes input from all 20 of S4\u2019s feature maps. The output layer has 9 outputs\n(since the face manifold is nine-dimensional) and is fully connected to C5 (such a full connection\ncan be seen as a convolution with 1\u00021 kernels).\n","The detection system operates on raw grayscale images. The convolutional network is applied to\nall 32\u0002 32 sub-windows of the image, stepped every 4 pixels horizontally and vertically. Because\nthe layers are convolutional, applying two replicas of the network in Figure 5 to two overlapping\ninput windows leads to a considerable amount of redundant computation. Eliminating the redundant\ncomputation yields a dramatic speed up: each layer of the convolutional network is extended so as\nto cover the entire input image. The output is also replicated the same way. Due to the two 2\u0002 2\nsubsampling layers, we obtain one output vector every 4\u00024 pixels.\n"],"Page":9,"Number":5,"Type":"Figure","CaptionBB":[123,355,724,371],"Height":1100,"Width":850,"DPI":100,"ImageBB":[168,129,669,326]}