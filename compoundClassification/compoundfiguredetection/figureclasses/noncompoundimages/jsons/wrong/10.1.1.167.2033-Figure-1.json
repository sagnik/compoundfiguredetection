{"Caption":"       Figure 1. Image examples of the UIUCTex dataset. classes with 40 images per class. Textures are viewed un- der significant scale and viewpoint changes. Non-rigid de- formations, illumination changes and viewpoint-dependent appearance variations are also present. The KTH-TIPS dataset [9] (Fig. 4) contains 10 texture classes with 81 im- ","ImageText":[],"Mention":["For our experimental evaluation, we use 4 texture and\n5 object category datasets, described below. For texture\nclassi\u0002cation, we randomly select 100 different training\/test\nsplits and report the average classi\u0002cation accuracy, to-\ngether with the standard deviation, over the 100 runs. For\nobject classi\u0002cation, we use the same training and test sets\nas the publications with which we are comparing.\nThe UIUCTex dataset [11] (Fig. 1) contains 25 texture\n","background class. We use the same training and test set for\ntwo-class classi\u0002cation as [16]. The CalTech101 dataset [6]\n(Fig. 2) contains 101 object categories. Most of the im-\nages in the database contain little or no clutter. Furthermore,\nthe objects tend to lie in the center of the image and to be\npresent in similar poses. Some images have a partially black\nbackground due to arti\u0002cial image rotations. We follow the\nexperimental setup of Grauman et al. [8], i.e., we randomly\nselect 30 training images per class and test on the remaining\nimages, reporting the average accuracy for all the classes.\nThe Pascal dataset [5] (Fig. 3) includes bicycles, cars, mo-\ntorbikes and people. It has one training dataset and two test\nsets. In the \u0093easier\u0094 test set 1, images are taken from the\nsame distribution as the training images. In the \u0093harder\u0094\ntest set 2, images are collected by Google search. An addi-\ntional complication is that many images in test set 2 contain\ninstances of several classes.\n"],"Page":3,"Number":1,"Type":"Figure","CaptionBB":[427,635,757,745],"Height":1169,"Width":826,"DPI":100,"ImageBB":[435,528,750,634]}