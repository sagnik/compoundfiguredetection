{"Caption":"Figure 1: Illustration of Sparse LSA (a) View of Matrix Factorization, white cells in A indicates the zero entries (b) View of document-topic-term relationship. ","ImageText":[{"Text":"(a)","TextBB":[590.679,174.943,608.36,189.472],"Rotation":0},{"Text":"(b)","TextBB":[590.29,412.982,608.741,427.511],"Rotation":0}],"Mention":["Another benefit of learning sparse A is to save com-\nputational cost and storage requirements when D is\nlarge. In traditional LSA, the topics with larger sin-\ngular values will cover a broader range of concepts than\nthe ones with smaller singular values. For example, the\nfirst few topics with largest singular values are often\ntoo general to have specific meanings. As singular val-\nues decrease, the topics become more and more spe-\ncific. Therefore, we might want to enlarge the number\nof latent topics D to have a reasonable coverage of the\ntopics. However, given a large corpus with millions of\ndocuments, a larger D will greatly increase the compu-\ntational cost of projection operations in traditional LSA.\nIn contrary, for Sparse LSA, projecting documents via a\nhighly sparse projection matrix will be computationally\nmuch more efficient; and it will take much less memory\nfor storing A when D is large.\nThe illustration of Sparse LSA from a matrix fac-\ntorization perspective is presented in Figure 1(a). An\nexample of topic-word relationship is shown in Figure\n1(b). Note that a latent topic (\u201Claw\u201D in this case) is\nonly related to a limited number of words.\nIn order to obtain a sparse A, inspired by the lasso\n"],"Page":3,"Number":1,"Type":"Figure","CaptionBB":[431,442,766,492],"Height":1169,"Width":826,"DPI":100,"ImageBB":[478,105,721,430]}