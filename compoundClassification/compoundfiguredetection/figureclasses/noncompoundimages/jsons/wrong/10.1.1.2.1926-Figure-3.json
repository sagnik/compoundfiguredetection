{"Caption":"Figure 3. Top and middle rows: scatter plots of error rates for all pairs of digits from the MNIST dataset. Top Left: POLA vs. Eu- clidean distance. Top Right: POLA vs FDA. Middle Left: POLA vs. RCA without PCA as a preprocessing step. Middle Right: POLA vs. RCA. Bottom row: the digits 0 and 8 after dimension- ality reduction using PCA (left) and POLA (right). ","ImageText":[{"Text":"0.5","TextBB":[258.688,84.0255,264.798,88.0919],"Rotation":0},{"Text":"0.4","TextBB":[109.762,101.223,115.873,105.289],"Rotation":0},{"Text":"0.4","TextBB":[258.688,101.223,264.798,105.289],"Rotation":0},{"Text":"POLA","TextBB":[251.132,127.572,257.457,145.817],"Rotation":3},{"Text":"error","TextBB":[251.132,111.235,257.457,125.671],"Rotation":3},{"Text":"POLA","TextBB":[102.207,127.572,108.532,145.817],"Rotation":3},{"Text":"error","TextBB":[102.207,111.235,108.532,125.671],"Rotation":3},{"Text":"0.5","TextBB":[109.762,84.0255,115.873,88.0919],"Rotation":0},{"Text":"0.3","TextBB":[109.762,118.42,115.873,122.486],"Rotation":0},{"Text":"0.2","TextBB":[109.762,135.619,115.873,139.685],"Rotation":0},{"Text":"0.1","TextBB":[109.762,152.816,115.873,156.882],"Rotation":0},{"Text":"0","TextBB":[113.426,170.034,115.87,174.1],"Rotation":0},{"Text":"0","TextBB":[115.359,173.189,117.804,177.256],"Rotation":0},{"Text":"0.3","TextBB":[258.688,118.42,264.798,122.486],"Rotation":0},{"Text":"0.2","TextBB":[258.688,135.619,264.798,139.685],"Rotation":0},{"Text":"0.1","TextBB":[258.688,152.816,264.798,156.882],"Rotation":0},{"Text":"0.1","TextBB":[135.345,173.189,141.456,177.256],"Rotation":0},{"Text":"0.2","TextBB":[157.164,173.189,163.275,177.256],"Rotation":0},{"Text":"0.3","TextBB":[178.981,173.189,185.091,177.256],"Rotation":0},{"Text":"0.4","TextBB":[200.799,173.189,206.909,177.256],"Rotation":0},{"Text":"0","TextBB":[262.351,170.034,264.796,174.1],"Rotation":0},{"Text":"0","TextBB":[264.285,173.189,266.729,177.256],"Rotation":0},{"Text":"0.5","TextBB":[222.617,173.189,228.727,177.256],"Rotation":0},{"Text":"0.1","TextBB":[284.271,173.189,290.381,177.256],"Rotation":0},{"Text":"0.2","TextBB":[306.089,173.189,312.2,177.256],"Rotation":0},{"Text":"kNN","TextBB":[156.349,177.582,169.643,183.907],"Rotation":0},{"Text":"error","TextBB":[171.544,177.582,185.98,183.907],"Rotation":0},{"Text":"0.3","TextBB":[327.906,173.189,334.016,177.256],"Rotation":0},{"Text":"0.4","TextBB":[349.724,173.189,355.834,177.256],"Rotation":0},{"Text":"0.5","TextBB":[371.542,173.189,377.652,177.256],"Rotation":0},{"Text":"FDA","TextBB":[305.071,177.582,318.748,183.907],"Rotation":0},{"Text":"error","TextBB":[320.649,177.582,335.085,183.907],"Rotation":0},{"Text":"0.5","TextBB":[109.762,190.251,115.873,194.317],"Rotation":0},{"Text":"0.3","TextBB":[258.212,200.034,264.474,204.201],"Rotation":0},{"Text":"POLA","TextBB":[247.967,232.416,254.449,251.112],"Rotation":3},{"Text":"error","TextBB":[247.967,215.675,254.449,230.468],"Rotation":3},{"Text":"POLA","TextBB":[102.207,233.797,108.532,252.042],"Rotation":3},{"Text":"error","TextBB":[102.207,217.46,108.532,231.896],"Rotation":3},{"Text":"0.4","TextBB":[109.762,207.448,115.873,211.514],"Rotation":0},{"Text":"0.3","TextBB":[109.762,224.645,115.873,228.711],"Rotation":0},{"Text":"0.2","TextBB":[109.762,241.844,115.873,245.91],"Rotation":0},{"Text":"0.25","TextBB":[255.71,212.693,264.476,216.86],"Rotation":0},{"Text":"0.2","TextBB":[258.212,225.332,264.474,229.499],"Rotation":0},{"Text":"0.15","TextBB":[255.71,237.971,264.476,242.138],"Rotation":0},{"Text":"0.1","TextBB":[258.212,250.631,264.474,254.798],"Rotation":0},{"Text":"0.1","TextBB":[109.762,259.041,115.873,263.107],"Rotation":0},{"Text":"0.05","TextBB":[255.71,263.27,264.476,267.437],"Rotation":0},{"Text":"0","TextBB":[113.426,276.259,115.87,280.325],"Rotation":0},{"Text":"0","TextBB":[115.359,279.414,117.804,283.481],"Rotation":0},{"Text":"0.1","TextBB":[135.345,279.414,141.456,283.481],"Rotation":0},{"Text":"0.2","TextBB":[157.164,279.414,163.275,283.481],"Rotation":0},{"Text":"0.3","TextBB":[178.981,279.414,185.091,283.481],"Rotation":0},{"Text":"RCA","TextBB":[155.779,283.807,170.215,290.132],"Rotation":0},{"Text":"error","TextBB":[172.116,283.807,186.552,290.132],"Rotation":0},{"Text":"0.4","TextBB":[200.799,279.414,206.909,283.481],"Rotation":0},{"Text":"0.5","TextBB":[222.617,279.414,228.727,283.481],"Rotation":0},{"Text":"0","TextBB":[261.967,275.929,264.471,280.096],"Rotation":0},{"Text":"0","TextBB":[263.949,279.161,266.453,283.328],"Rotation":0},{"Text":"0.05","TextBB":[276.858,279.161,285.625,283.328],"Rotation":0},{"Text":"0.1","TextBB":[294.149,279.161,300.41,283.328],"Rotation":0},{"Text":"0.15","TextBB":[308.935,279.161,317.701,283.328],"Rotation":0},{"Text":"0.2","TextBB":[326.225,279.161,332.487,283.328],"Rotation":0},{"Text":"0.25","TextBB":[341.012,279.161,349.779,283.328],"Rotation":0},{"Text":"0.3","TextBB":[358.301,279.161,364.563,283.328],"Rotation":0},{"Text":"RCA","TextBB":[305.369,283.663,320.163,290.145],"Rotation":0},{"Text":"error","TextBB":[322.111,283.663,336.904,290.145],"Rotation":0}],"Mention":["the first experiment we compared the performance of kNN\nusing the Euclidean distance to its performance when using\na pseudo-metric obtained by running POLA on the training\nset. To train POLA we randomly chose 1; 000 pairs of in-\nstances and used the last hypothesis generated by POLA\nfor evaluation on the test set (see Sec. 6). A comparison of\nthe error on all 45 binary classification problems is given\non the top left scatter plot of Fig. 3. Each point in the plot\ncorresponds to a binary classification problem. The x-axis\ndesignates the error of kNN with Euclidean distance while\nthe y-axis is the error of kNN using POLA\u2019s pseudo-metric.\nIt is clear that using the learned pseudo-metric greatly re-\nduces the error rate. In fact, the error when using POLA as\na pre-processing step is lower than the vanilla kNN in all of\nthe 45 binary problems. Next, we compared the RCA algo-\nrithm for learning distances (Shental et al., 2002) to POLA.\nRCA follows the same learning setting as POLA in a batch\nmode. We compared the performance of kNN using a dis-\ntance function learned by RCA to its performance using a\npseudo-metric learned by POLA. RCA uses PCA as a pre-\nprocessing step in order to reduce dimensionality. We thus\napplied PCA independently to each binary problem and re-\nduced the dimension of each 282 image to a 40 dimensional\nvector. This value of the dimension was chosen by exper-\nimentation on the test set. The results, comparing POLA\nwith RCA, are given on the middle right plot of Fig. 3.\nPOLA outperforms RCA on all but one of the 45 binary\nproblems. We also applied RCA without the dimensional-\nity reduction step. The results are given on the middle left\nplot of Fig. 3. Here, the results of RCA are much worse\nand POLA outperforms RCA on all of the 45 binary prob-\nlems. The fact that POLA does not require dimensionality\nreduction is in accordance with our formal analysis. In-\ndeed, the loss bound of Thm. 1 depends on the Frobenius\nnorm of A? and does not depend on the actual dimension\nof the instances.\n","of Fig. 3. Here again kNN with POLA clearly outperforms\nkNN with FDA on all of the problems.\n","In the final experiment with the MNIST dataset we ran-\ndomly selected 100 images corresponding to the digits \u201D0\u201D\nand \u201D8\u201D. We then projected each image onto the two largest\neigenvectors obtained by PCA and the two largest eigen-\nvectors of the matrix learned by POLA. The two projec-\ntions are shown in the bottom row of Fig. 3 The projected\npoints using the eigenvectors obtained by POLA generated\ntwo perfectly separable clusters each of which corresponds\nto a different digit. In contrast the analogous clusters when\nusing PCA are interleaved. This demonstrates the potential\npower of POLA for dimensionality reduction.\n"],"Page":7,"Number":3,"Type":"Figure","CaptionBB":[75,414,402,503],"Height":1100,"Width":850,"DPI":100,"ImageBB":[99,81,380,396]}