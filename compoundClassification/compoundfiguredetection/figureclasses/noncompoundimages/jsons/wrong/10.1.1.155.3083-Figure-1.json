{"Caption":"Figure 1: Illustration of the scene matching problem. Left: Input image (along with the output segmentation given by our system overlaid) to be matched to a dataset of 100k street images. Notice that the output segment boundaries align well with the depicted objects in the scene. Top right: top three retrieved images, based on matching the gist descriptor [14] over the entire image. The matches are not good. Bottom right: Searching for matches within each estimated segment (using the same gist representation within the segment) and compositing the results yields much better matches to the input image. ","ImageText":[],"Mention":["However, as noted by several authors and illustrated in Figure 1, the major stumbling block of all\nthe scene-matching approaches is that, despite the large quantities of data, for many types of im-\nages the quality of the matches is still not very good. Part of the reason is that the low-level image\ndescriptors used for matching are just not powerful enough to capture some of the more semantic\nsimilarity. Several approaches have been proposed to address this shortcoming, including syntheti-\ncally increasing the dataset with transformed copies of images [22], cleaning matching results using\nclustering [18, 7, 5], automatically prefiltering the dataset [21], or simply picking good matches by\nhand [6]. All these appraoches improve performance somewhat but don\u2019t alleviate this issue entirely.\nWe believe that there is a more fundamental problem \u2013 the variability of the visual world is just so\n"],"Page":2,"Number":1,"Type":"Figure","CaptionBB":[148,293,698,399],"Height":1169,"Width":826,"DPI":100,"ImageBB":[148,111,702,283]}