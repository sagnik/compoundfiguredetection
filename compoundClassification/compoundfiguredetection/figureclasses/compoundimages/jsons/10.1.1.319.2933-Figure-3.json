{"Caption":"Figure 3: Comparison between the best performing set of features (ig600) and: (a) two baselines: lead of the meeting (m-lead) and lead of each topical segment (t-lead); the best predictor alone (duration). (b) a set of features not depending on lexical information (struct+duration+acoustic), a set of features with only lexical and word frequencies (lexical), the best set of features without discourse features (no-discourse). ","ImageText":[{"Text":"Baselines","TextBB":[295.067,153,337.027,161.954],"Rotation":0},{"Text":"m−lead","TextBB":[365.968,178.328,397.978,187.281],"Rotation":0},{"Text":"t−lead","TextBB":[365.968,190.267,392.606,199.22],"Rotation":0},{"Text":"duration","TextBB":[365.968,202.285,400.94,211.238],"Rotation":0},{"Text":"ig600","TextBB":[365.968,214.224,389.644,223.177],"Rotation":0},{"Text":"0.6","TextBB":[206.418,212.851,219.873,221.805],"Rotation":0},{"Text":"0.5","TextBB":[206.418,261.572,219.873,270.526],"Rotation":0},{"Text":"0.5","TextBB":[451.956,261.572,465.41,270.526],"Rotation":0},{"Text":"0.4","TextBB":[206.418,310.293,219.873,319.247],"Rotation":0},{"Text":"0.4","TextBB":[451.956,310.293,465.41,319.247],"Rotation":0},{"Text":"0.3","TextBB":[206.418,359.013,219.873,367.966],"Rotation":0},{"Text":"0.3","TextBB":[451.956,359.013,465.41,367.966],"Rotation":0},{"Text":"0.2","TextBB":[206.418,407.733,219.873,416.687],"Rotation":0},{"Text":"0.2","TextBB":[451.956,407.733,465.41,416.687],"Rotation":0},{"Text":"0.1","TextBB":[206.418,456.454,219.873,465.408],"Rotation":0},{"Text":"0.1","TextBB":[451.956,456.454,465.41,465.408],"Rotation":0},{"Text":"0","TextBB":[214.483,505.174,219.865,514.127],"Rotation":0},{"Text":"0.2","TextBB":[251.669,513.321,265.124,522.274],"Rotation":0},{"Text":"0.4","TextBB":[289.419,513.321,302.874,522.274],"Rotation":0},{"Text":"0.6","TextBB":[327.089,513.321,340.543,522.274],"Rotation":0},{"Text":"recall","TextBB":[304.342,524.856,327.466,533.809],"Rotation":0},{"Text":"0.8","TextBB":[364.758,513.321,378.213,522.274],"Rotation":0},{"Text":"struct+duration+acoustic","TextBB":[541.571,178.328,647.02,187.281],"Rotation":0},{"Text":"lexical","TextBB":[541.571,190.267,568.461,199.22],"Rotation":0},{"Text":"no−discourse","TextBB":[541.571,202.285,599.406,211.238],"Rotation":0},{"Text":"ig600","TextBB":[541.571,214.224,565.247,223.177],"Rotation":0},{"Text":"0.6","TextBB":[451.956,212.851,465.41,221.805],"Rotation":0},{"Text":"precision","TextBB":[438.39,318.604,447.344,357.332],"Rotation":3},{"Text":"precision","TextBB":[192.853,318.604,201.806,357.332],"Rotation":3},{"Text":"Feature","TextBB":[534.715,153,568.071,161.954],"Rotation":0},{"Text":"sets","TextBB":[570.762,153,588.514,161.954],"Rotation":0},{"Text":"0","TextBB":[460.021,505.174,465.403,514.127],"Rotation":0},{"Text":"1","TextBB":[406.542,513.321,411.923,522.274],"Rotation":0},{"Text":"0.2","TextBB":[497.207,513.321,510.661,522.274],"Rotation":0},{"Text":"0.4","TextBB":[534.957,513.321,548.411,522.274],"Rotation":0},{"Text":"0.6","TextBB":[572.626,513.321,586.081,522.274],"Rotation":0},{"Text":"recall","TextBB":[549.879,524.856,573.004,533.809],"Rotation":0},{"Text":"0.8","TextBB":[610.296,513.321,623.75,522.274],"Rotation":0},{"Text":"1","TextBB":[652.079,513.321,657.461,522.274],"Rotation":0}],"Mention":["ground-truth feature values of DAs for the three different predictors discussed in the previous sub-\nsection, i.e. prev, ssprev, and addreprev. This is just to assess to potential benefit of DA features,\nand may of course not be indicative of real performance. I selected 54 meetings that have been\nannotated with DA tags and used to build extractive summaries. The dataset represents 85,756\ninstances, of which 4,999 are positive instances (i.e. 5.83%); 4,000 were used for development,\n10,000 for testing, and the rest for training. In order to have a basis for comparison, I augmented\nthe discourse-level features discussed previously with features that have been found useful in text\nsummarization tasks: frequency, \u0000\u0002\u0001 \u0003\t\u0005 \u0007\t\u0001 , positional features, and so on (see Table 7). I trained\nmodels for different groupings of features: structural, lexical, durational, acoustic, global statis-\ntics, positional, and discourse. Since features sets were over-generated and contained too many\nunpredictive and mutually correlated features, I ran on each grouping a correlation-based feature\nsubset selection algorithm based on information gain [Hall, 1998] that seeks to select features that\nare highly correlated with the class, yet seldom correlated with other features. I used this method\nto restrict each of the groupings mentioned above to a maximum of 600 features (this had only\nan impact on sets that contained lexical and discourse features, which are quite numerous). It was\ninteresting to notice that, in the feature set that combined all features (ig600), 84 of the features\nthat were selected are discourse features.\nI trained a log-linear model for each feature grouping. Since the data is particularly skewed\ntoward negative examples, no data set outperformed the majority-class baseline in classification\naccuracy. Performance is summarized in Figure 3 and Table 8 (ig600 refers to the 600 most pre-\ndictive features across all sets). The main finding provided by table lies in the fact that the ig600\nfeature set (containing discourse features as well) performs significantly better than the set of all\nfeatures except discourse (which also contained the same number of features).\nOverall, summarization results are admittedly currently quite poor, especially if we consider\nthe fact that best achieved F-measure (.251) used labels that are difficult to obtain automatically.\nIt is however generally agreed that precision\/recall evaluation is a too strict for summarization\npurposes, and doesn\u2019t account for cases where two or more utterances may be good alternatives\nfor inclusion in a summary (similar propositional content), and only one of them was included\nin the gold standard summary. I plan to experiment with automatic evaluation metrics specific to\nsummarization, such as the Pyramid method [Nenkova and Passonneau, 2004] or ROUGE [Lin,\n2004].\n","The proposed utterance revision system is aimed at condensing utterances by removing non-\nfluencies typical to spontaneous speech, as well as semantically empty phrases (\u201CI mean\u201D), low-\ncontent and grammatically optional constituents. It incorporates various knowledge sources: syn-\ntactic transformation rules automatically acquired from transcription-abstract parallel texts; a syntax-\nbased language model to promote revision hypotheses that are grammatical; a phrase-based dele-\ntion model that was trained to identify phrases likely to be deleted (\u201Cyou know\u201D); a \u0000\u0002\u0001\u0004\u0003 \u0005\b\u0007\t\u0001 model\nof word importance, to promote the removal of low information content constituents, and prosodic\nfeatures to exploit any existing acoustic correlates of words found in abstracts, e.g. prosodically\nstressed words. The different knowledge sources are integrated in a probabilistic framework where\nthe available evidence of all models is combined to select a globally optimal analysis. A dynamic\nprogramming algorithm is used to ensure that the different possible analyzes are scored and ranked\nin an efficient manner.\nMore specifically, the proposed utterance revision model is based on general formalisms known\nas synchronous grammars or transformational grammars, which are designed to generate two lan-\nguages synchronously (previous work is particularly extensive; see, e.g., [Shieber and Schabes,\n1990]). A rule of a synchronous grammar may for example correspond to the deletion of an ad-\nverb, as it is the case in Figure 4(3) (more rigorous definitions will be introduced in Section 5.4).\nSuch formalisms have generated substantial interest in other areas of NLP, particularly machine\ntranslation, but there are relatively few previous applications to summarization. The proposed\nmodel is stochastic and fully trainable from any bi-text of sentence-aligned utterances and revi-\nsions, assuming that syntax trees (possibly automatically constructed) are available on both sides.\nThe advantage of modeling the transformation between a transcription sentence and a revised\n","The representations of revision rules that have been commonly used in automatically trainable\nsyntax-based sentence condensation systems ( [Knight and Marcu, 2000; Knight and Marcu, 2002;\nDaume´ III and Marcu, 2002; Reizler et al., 2003; Turner and Charniak, 2005], which I will hence-\nforth call child-deletion models) are restricted instance of synchronous context free grammars\n(S-CFG), also known as syntax-directed transduction grammars [Aho and Ullman, 1969] (essen-\ntially two context free grammars working in pair), a formalism that can only account for quite\nlimited types of syntactic divergences between tree pairs (in Figure 4, a child-deletion models can-\nnot transform tree (1) into tree (2)). Given their incapacity to model the transformation between a\nsentence and its revision in most real-world examples, it is arguably desirable to switch to richer\nmodels such as synchronous tree substitution grammars (S-TSG) or synchronous tree adjoining\ngrammars (S-TAG), which have been used in other fields of NLP, e.g. [Shieber and Schabes, 1990]\nin machine translation. These models are generalization of S-CFGs in that rules represent trans-\nformations between two arbitrary synchronized trees (Figure 4(3) presents an example of such a\nrule). The extended domain of locality pertaining to S-TSG allows us to model the transformation\nbetween any tree pair as a sequence of revision rules, which is computationally appealing since it\nallows us to exploit any parallel corpus in its entirety (as opposed to only 1.7% of the data corpus\nsuch as Ziff-Davis).\nI restricted my first experiments to the extraction of S-TSG rules, though I do not exclude the\npossibility of extracting and using S-TAG rules in the future. Training a synchronous grammar\ncomprises here two subproblems: extract rules from a parallel treebank and assign probabilities to\nthem. These two problems are addressed in the two next subsections (5.4.1 and 5.4.2).\n"],"Page":22,"Number":3,"Type":"Figure","CaptionBB":[98,556,749,628],"Height":1100,"Width":850,"DPI":100,"ImageBB":[181,140,668,543]}