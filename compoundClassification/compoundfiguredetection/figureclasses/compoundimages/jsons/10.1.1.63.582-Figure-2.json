{"Caption":"Figure 2: Policy value and misclassification cost in the grid- world experiment, with standard deviations plotted. ","ImageText":[{"Text":"    ","TextBB":[109.018,75.5728,126.687,84.6105],"Rotation":0},{"Text":"    ","TextBB":[109.018,101.36,126.687,110.397],"Rotation":0},{"Text":"    ","TextBB":[109.018,127.147,126.687,136.184],"Rotation":0},{"Text":"539","TextBB":[99.7066,148.792,109.667,163.424],"Rotation":3},{"Text":"   ","TextBB":[113.449,152.934,126.7,161.971],"Rotation":0},{"Text":"    ","TextBB":[109.018,178.721,126.687,187.758],"Rotation":0},{"Text":"&,","TextBB":[348.478,191.524,359.09,202.38],"Rotation":0},{"Text":"    ","TextBB":[109.018,204.508,126.687,213.545],"Rotation":0},{"Text":"&6","TextBB":[348.478,217.364,359.09,228.221],"Rotation":0},{"Text":"    ","TextBB":[109.018,230.295,126.687,239.332],"Rotation":0},{"Text":" ","TextBB":[131.542,243.004,135.959,252.042],"Rotation":0},{"Text":"    ","TextBB":[172.212,243.004,189.881,252.042],"Rotation":0},{"Text":"     ","TextBB":[217.366,243.004,239.452,252.042],"Rotation":0},{"Text":"     ","TextBB":[264.696,243.004,286.782,252.042],"Rotation":0},{"Text":"     ","TextBB":[312.052,243.004,334.139,252.042],"Rotation":0},{"Text":"     ","TextBB":[359.382,243.004,381.469,252.042],"Rotation":0},{"Text":"\/HDUQLQJ 7ULDOV","TextBB":[215.483,253.042,288.507,263.003],"Rotation":0},{"Text":"    ","TextBB":[110.978,263.84,129.059,273.089],"Rotation":0},{"Text":"    ","TextBB":[110.978,278.5,129.059,287.749],"Rotation":0},{"Text":"&,","TextBB":[346.789,274.714,357.648,285.824],"Rotation":0},{"Text":"   ","TextBB":[115.512,293.16,129.073,302.408],"Rotation":0},{"Text":"&6","TextBB":[346.789,296.65,357.648,307.759],"Rotation":0},{"Text":"    ","TextBB":[110.978,307.82,129.059,317.068],"Rotation":0},{"Text":"0&&","TextBB":[97.3232,332.901,107.516,347.874],"Rotation":3},{"Text":"    ","TextBB":[110.978,322.48,129.059,331.728],"Rotation":0},{"Text":"    ","TextBB":[110.978,337.14,129.059,346.388],"Rotation":0},{"Text":"    ","TextBB":[110.978,351.8,129.059,361.048],"Rotation":0},{"Text":"   ","TextBB":[115.512,366.46,129.073,375.708],"Rotation":0},{"Text":"    ","TextBB":[110.978,381.12,129.059,390.368],"Rotation":0},{"Text":"    ","TextBB":[110.978,395.779,129.059,405.028],"Rotation":0},{"Text":"    ","TextBB":[110.978,410.439,129.059,419.688],"Rotation":0},{"Text":" ","TextBB":[134.027,423.446,138.547,432.694],"Rotation":0},{"Text":"    ","TextBB":[173.881,423.446,191.961,432.694],"Rotation":0},{"Text":"     ","TextBB":[218.295,423.446,240.896,432.694],"Rotation":0},{"Text":"     ","TextBB":[264.991,423.446,287.592,432.694],"Rotation":0},{"Text":"     ","TextBB":[311.659,423.446,334.26,432.694],"Rotation":0},{"Text":"     ","TextBB":[358.327,423.446,380.928,432.694],"Rotation":0},{"Text":"\/HDUQLQJ 7ULDOV","TextBB":[215.499,437.817,290.225,448.009],"Rotation":0}],"Mention":["to be the reason for a more rapid RPV improvement exhib-\nited by CS.\nFinally, we find that the standard deviation of the policies\nobtained by CS is lower (Figure 2(a)). This seems to be due\nto the fact that CS pays more attention to more important\nstates.\n","The true objective of our reinforcement learning agent is to\nmaximize RPV while the classifier\u2019s objective is to reduce\nMCC.\nIn the experiments, T contained 1000 states randomly\nsampled from the original state space: 700 of them were\nused for training, and the other 300 were used for validation\nto guard against overfitting. Ten random 100 Ã— 100 grid-\nworlds were generated according to the reward distribution,\nand 20 learning sessions were conducted in each of them re-\nsulting in a total of 400 trials for each algorithm. The results\nare plotted in Figures 2 with standard deviation as the error\nbars. Several observations are in order.\nFirst, note that the importance-sensitive algorithm CS\nincreased the policy value substantially faster than the\nimportance-insensitive CI. The significant advantage was\nobserved early in the learning process. Note that the im-\nportance values of the grid-world states vary significantly,\nas shown in Figure 1(b). For many states, the importance is\nnegligible or even zero; on the other hand, some of the states\nhave much greater importance which makes them more sig-\nnificant in affecting the policy value and deserve more at-\ntention in learning. This observation leads to the conjecture\nthat CS can learn even better when there is a high variance\nin the importance distribution over the state space.\nSecond, we note that the importance-sensitive learner CS\n"],"Page":5,"Number":2,"Type":"Figure","CaptionBB":[73,452,406,482],"Height":1100,"Width":850,"DPI":100,"ImageBB":[95,74,383,449]}