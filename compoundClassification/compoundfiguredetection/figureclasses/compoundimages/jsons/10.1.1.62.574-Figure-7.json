{"Caption":"Figure 7: Bandwidth fluctuation smoothed by Fast Cache","ImageText":[{"Text":"0.8","TextBB":[323.187,106.755,333.85,113.85],"Rotation":0},{"Text":"0.6","TextBB":[323.187,132.55,333.85,139.646],"Rotation":0},{"Text":"CDF","TextBB":[311.718,140.498,318.814,156.263],"Rotation":3},{"Text":"CDF","TextBB":[77.3859,140.498,84.4818,156.263],"Rotation":3},{"Text":"0.95","TextBB":[89.191,113.211,104.119,120.307],"Rotation":0},{"Text":"1","TextBB":[329.584,80.992,333.849,88.088],"Rotation":0},{"Text":"1","TextBB":[99.8535,80.992,104.119,88.088],"Rotation":0},{"Text":"0.9","TextBB":[93.4564,145.431,104.119,152.527],"Rotation":0},{"Text":"0.4","TextBB":[323.187,158.311,333.85,165.407],"Rotation":0},{"Text":"0.85","TextBB":[89.191,177.65,104.119,184.746],"Rotation":0},{"Text":"0.2","TextBB":[323.187,184.106,333.85,191.202],"Rotation":0},{"Text":"Fast","TextBB":[216.311,196.348,231.231,203.444],"Rotation":0},{"Text":"Cache","TextBB":[233.364,196.348,255.534,203.444],"Rotation":0},{"Text":"Normal","TextBB":[213.342,204.02,238.059,211.116],"Rotation":0},{"Text":"TCP","TextBB":[240.191,204.02,255.534,211.116],"Rotation":0},{"Text":"0.8","TextBB":[93.4564,209.87,104.119,216.966],"Rotation":0},{"Text":"0","TextBB":[107.655,217.541,111.92,224.637],"Rotation":0},{"Text":"0.2","TextBB":[140.672,217.541,151.335,224.637],"Rotation":0},{"Text":"0.4","TextBB":[176.887,217.541,187.55,224.637],"Rotation":0},{"Text":"0.6","TextBB":[213.069,217.541,223.732,224.637],"Rotation":0},{"Text":"Fast","TextBB":[450.643,196.348,465.564,203.444],"Rotation":0},{"Text":"Cache","TextBB":[467.696,196.348,489.866,203.444],"Rotation":0},{"Text":"Normal","TextBB":[447.674,204.02,472.391,211.116],"Rotation":0},{"Text":"TCP","TextBB":[474.523,204.02,489.866,211.116],"Rotation":0},{"Text":"0","TextBB":[329.584,209.87,333.849,216.966],"Rotation":0},{"Text":"0.8","TextBB":[249.284,217.541,259.947,224.637],"Rotation":0},{"Text":"1","TextBB":[288.698,217.541,292.963,224.637],"Rotation":0},{"Text":"1","TextBB":[337.384,217.541,341.649,224.637],"Rotation":0},{"Text":"Rebuffer","TextBB":[130.826,229.048,160.245,236.144],"Rotation":0},{"Text":"Ratio","TextBB":[162.378,229.048,180.283,236.144],"Rotation":0},{"Text":"(rebuffer","TextBB":[182.415,229.048,211.405,236.144],"Rotation":0},{"Text":"time","TextBB":[213.538,229.048,228.029,236.144],"Rotation":0},{"Text":"\/","TextBB":[230.161,229.048,232.294,236.144],"Rotation":0},{"Text":"play","TextBB":[234.426,229.048,248.496,236.144],"Rotation":0},{"Text":"time)","TextBB":[250.628,229.048,267.674,236.144],"Rotation":0},{"Text":"10","TextBB":[381.663,217.541,390.194,224.637],"Rotation":0},{"Text":"100","TextBB":[425.942,217.541,438.738,224.637],"Rotation":0},{"Text":"1000","TextBB":[470.222,217.541,487.282,224.637],"Rotation":0},{"Text":"10000","TextBB":[514.501,217.541,535.827,224.637],"Rotation":0},{"Text":"Transmission","TextBB":[383.324,229.048,428.929,236.144],"Rotation":0},{"Text":"Duration","TextBB":[431.062,229.048,460.052,236.144],"Rotation":0},{"Text":"(sec)","TextBB":[462.184,229.048,479.23,236.144],"Rotation":0},{"Text":"(a)","TextBB":[129.833,240.993,146.153,253.447],"Rotation":0},{"Text":"Rebuffering","TextBB":[150.341,240.993,215.066,253.447],"Rotation":0},{"Text":"ratio","TextBB":[219.166,240.993,245.399,253.447],"Rotation":0},{"Text":"(b)","TextBB":[334.5,240.993,351.652,253.447],"Rotation":0},{"Text":"Data","TextBB":[355.841,240.993,383.413,253.447],"Rotation":0},{"Text":"transmission","TextBB":[387.68,240.993,457.265,253.447],"Rotation":0},{"Text":"duration","TextBB":[461.82,240.993,509.432,253.447],"Rotation":0}],"Mention":["encoding rates of Windows and RealNetworks media in our\nworkloads are comparable. Figure 4(c) further compares\nthe delay from the session beginning time to the transport\nsetup completion time for sessions with and without pro-\ntocol rollover in Windows media services in the home user\nworkload. As shown in the figure, about 37% of the sessions\nwith protocol rollover have a delay longer than 5 seconds.\nIn contrast, only about 13% of the sessions without protocol\nrollover have a delay longer than 5 seconds.\n","bility of encountering network congestion by reducing the\ndata transmission time. Figure 7(b) shows the distribu-\ntion of data transmission duration for Fast Cache supported\nstreaming sessions and normal TCP streaming sessions, re-\nspectively. We can see that although in general the media\nobjects streamed with Fast Cache have higher file lengths\nand encoding rates as shown in Figure 5, the transmission\ntime of Fast Cache supported streaming is much shorter\nthan that of normal TCP streaming (note that the x-axis is\nin log scale).\n","Fast Cache delivers a media object to a client faster than\nthe playing speed by over-utilizing the bandwidth and CPU\nresources. However, streaming a media object at a rate\nhigher than its encoding rate is only possible when the\navailable bandwidth between a client and its server is large\nenough. Intuitively, when a media object is streamed at its\nencoding rate, the higher the average bandwidth between a\nclient and its server over its encoding rate, the lower possi-\nbility at which performance degradation occurs during the\nplayback. To understand whether Fast Cache performs bet-\nter than normal TCP-based streaming when the average\nbandwidth between a client and its server is large enough,\nwe plot the CDF of rebuffering ratio for Fast Cache based\nstreaming sessions and normal TCP-based streaming ses-\nsions in the home user workload in which the media encoding\nrate of each stream is 200\u2013320 Kbps and the client adver-\ntised bandwidth (extracted from the Bandwidth header) is\nat least 500 Kbps greater than the media encoding rate, as\nshown in Figure 12. Compared with Figure 7(a), the two\ncurves in Figure 12 are very close, which means that, al-\nthough temporary network congestion may occur from time\nto time, a small play-out buffer performs well enough to\nsmooth out bandwidth fluctuation during streaming, when\nthe average bandwidth is large enough. Thus, aggressively\nover-utilizing the server and Internet resources is neither\nperformance-effective nor cost-efficient under a high band-\nwidth condition. The higher speed at which Fast Cache can\nstream a media object, the lower necessity is this speed for\na client. Furthermore, even if no extra traffic generated (as-\n","respectively. We can see that for sessions with longer dura-\ntions, degradation happens with a higher probability. For\nexample, in the business user workload, 88% of the sessions\nwith quality degradations have a duration longer than 100\nseconds, while 58% of the sessions without quality degra-\ndations have a duration longer than 100 seconds. Table\n5 further shows the breakdowns of sessions with and with-\nout quality degradations for TCP-based video streaming ses-\nsions that are longer than 30 seconds and longer than 300\nseconds, in the home and business user workloads, respec-\ntively. We can see that quality degradation happens less\nfrequently in the home user workload than in the business\nuser workload, which may be due to the longer playback\nduration of business users as shown in Figure 17. For ses-\nsions longer than 30 seconds, 13%\u201340% of the video sessions\nstill have quality degradation due to the rebuffering, stream\nswitch, stream thinning, and video cancellation. For ses-\nsions longer than 300 seconds, the quality is getting worse.\nFurther investigation shows that in a significant amount of\nvideo sessions with rebuffering, the requested media objects\nare MBR encoded, and the lack of stream switch is largely\ndue to the usage of Fast Cache, which disables rate adapta-\ntion.\nIn conclusion, the quality of media streaming on the Inter-\nnet leaves much to be improved, especially for those sessions\nwith longer durations.\n","According to our extensive trace analysis and real ex-\nperiments, Fast Cache does not support rate adaptation in\npractice. In a streaming session with Fast Cache enabled,\nthe client never requests to switch streams after the ini-\ntial stream selection in the SETUP command, even if there\nis a more suitable stream matching the decreased\/increased\nbandwidth during playback. Thinning and video cancel-\nlation are also disabled when Fast Cache is enabled. As\na result, when the bandwidth drops below the encoding\nrate, Fast Cache supported streaming performs like pseudo\nstreaming [17]: the player stops to buffer data for a while,\nthen continues to play the media for about five seconds (the\nplay-out buffer size), and this procedure repeats. With such\na configuration, if a sudden network congestion happens and\nlasts for a long time, the streaming quality of Fast Cache\nsupported streaming could be even worse than that of nor-\nmal TCP streaming. Figure 16 shows that when rebuffering\nhappens, the rebuffering duration of Fast Cache supported\nstreaming is much longer than that of normal TCP stream-\ning in the home user workload, because it cannot switch to\na lower rate stream upon network congestion.\nFigure 17(a) and Figure 17(b) show the CDF of play-\nback duration of TCP-based video streaming sessions that\nare longer than 30 seconds in the home and business user\nworkloads, respectively. The three curves in each figure\ndenote all sessions, sessions without quality degradations,\nand sessions with quality degradations (including rebuffer-\ning, stream switch, stream thinning, and video cancellation),\n"],"Page":9,"Number":7,"Type":"Figure","CaptionBB":[116,268,491,281],"Height":1100,"Width":850,"DPI":100,"ImageBB":[75,79,538,256]}