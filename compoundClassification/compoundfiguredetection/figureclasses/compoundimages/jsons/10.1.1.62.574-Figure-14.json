{"Caption":"Figure 14: Stream switch","ImageText":[{"Text":"0.8","TextBB":[85.9475,416.061,94.6966,421.883],"Rotation":0},{"Text":"0.8","TextBB":[264.447,416.061,273.196,421.883],"Rotation":0},{"Text":"0.6","TextBB":[85.9475,435.163,94.6966,440.985],"Rotation":0},{"Text":"0.6","TextBB":[264.447,435.163,273.196,440.985],"Rotation":0},{"Text":"0.4","TextBB":[85.9475,454.288,94.6966,460.11],"Rotation":0},{"Text":"0.4","TextBB":[264.447,454.288,273.196,460.11],"Rotation":0},{"Text":"0.2","TextBB":[85.9475,473.389,94.6966,479.211],"Rotation":0},{"Text":"0.2","TextBB":[264.447,473.389,273.196,479.211],"Rotation":0},{"Text":"Biz","TextBB":[199.568,481.546,208.311,487.369],"Rotation":0},{"Text":"Home","TextBB":[191.524,487.842,208.311,493.664],"Rotation":0},{"Text":"0","TextBB":[91.1965,492.514,94.6962,498.336],"Rotation":0},{"Text":"0.1","TextBB":[94.9736,498.808,103.723,504.631],"Rotation":0},{"Text":"1","TextBB":[143.523,498.808,147.023,504.631],"Rotation":0},{"Text":"10","TextBB":[187.722,498.808,194.721,504.631],"Rotation":0},{"Text":"100","TextBB":[231.896,498.808,242.395,504.631],"Rotation":0},{"Text":"(a)","TextBB":[87.5006,518.993,103.821,531.447],"Rotation":0},{"Text":"Stream","TextBB":[108.176,518.993,148.006,531.447],"Rotation":0},{"Text":"switch","TextBB":[152.34,518.993,187.443,531.447],"Rotation":0},{"Text":"latency","TextBB":[191.832,518.993,231.915,531.447],"Rotation":0},{"Text":"Biz","TextBB":[378.067,481.546,386.81,487.369],"Rotation":0},{"Text":"Home","TextBB":[370.024,487.842,386.811,493.664],"Rotation":0},{"Text":"0","TextBB":[269.696,492.514,273.195,498.336],"Rotation":0},{"Text":"Stream","TextBB":[115.43,508.25,135.716,514.072],"Rotation":0},{"Text":"Switch","TextBB":[137.466,508.25,156.003,514.072],"Rotation":0},{"Text":"Handoff","TextBB":[157.753,508.25,179.796,514.072],"Rotation":0},{"Text":"Latency","TextBB":[181.546,508.25,203.589,514.072],"Rotation":0},{"Text":"(sec)","TextBB":[205.338,508.25,219.324,514.072],"Rotation":0},{"Text":"1","TextBB":[269.696,396.936,273.195,402.758],"Rotation":0},{"Text":"1","TextBB":[91.1965,396.936,94.6962,402.758],"Rotation":0},{"Text":"CDF","TextBB":[255.021,440.694,260.843,453.629],"Rotation":3},{"Text":"CDF","TextBB":[76.5218,440.694,82.344,453.629],"Rotation":3},{"Text":"0.1","TextBB":[273.473,498.808,282.223,504.631],"Rotation":0},{"Text":"1","TextBB":[310.547,498.808,314.047,504.631],"Rotation":0},{"Text":"10","TextBB":[343.247,498.808,350.246,504.631],"Rotation":0},{"Text":"100","TextBB":[375.946,498.808,386.445,504.631],"Rotation":0},{"Text":"Stream","TextBB":[283.264,508.25,303.551,514.072],"Rotation":0},{"Text":"Switch:","TextBB":[305.3,508.25,325.587,514.072],"Rotation":0},{"Text":"Duration","TextBB":[327.337,508.25,351.123,514.072],"Rotation":0},{"Text":"of","TextBB":[352.873,508.25,358.123,514.072],"Rotation":0},{"Text":"Low","TextBB":[359.873,508.25,371.416,514.072],"Rotation":0},{"Text":"Quality","TextBB":[373.166,508.25,392.754,514.072],"Rotation":0},{"Text":"(sec)","TextBB":[394.504,508.25,408.49,514.072],"Rotation":0},{"Text":"(b)","TextBB":[268.667,518.993,285.819,531.447],"Rotation":0},{"Text":"Low","TextBB":[289.841,518.993,313.079,531.447],"Rotation":0},{"Text":"quality","TextBB":[317.501,518.993,356.239,531.447],"Rotation":0},{"Text":"duration","TextBB":[360.484,518.993,408.096,531.447],"Rotation":0},{"Text":"1000","TextBB":[408.646,498.808,422.644,504.631],"Rotation":0}],"Mention":["to the highest encoding rate that the content is transmitted\nin this session).\nAssuming a five-second play-out buffer [1], Figure 14(a)\nand Figure 14(b) show the distribution of stream switch la-\ntency and low quality duration in the home and business\nuser workloads, respectively. As shown in Figure 14(a),\nabout 30%\u201340% of the stream switches have a switch la-\ntency greater than 3 seconds, and about 10%\u201320% of the\nstream switches have a switch latency greater than 5 sec-\nonds, which is non-trivial for end users. In Figure 14(b), we\nobserve that about 60% of the sessions have a low quality\nduration less than 30 seconds, and 85% of the low quality\nstream durations are shorter than 40 seconds.\n","upper bound and a lower bound are applied to the play-out\nbuffer of the client player. The upper bound setting prevents\naggressive data buffering while the lower bound setting elim-\ninates the stream switch latency. When a streaming session\nstarts, the server transmits data to the client as fast as possi-\nble until the lower bound is reached. Then the playback be-\ngins and the client continues to buffer data with the highest\npossible rate until the buffer reaches its upper bound. With\na full buffer, the client buffers data at the media encoding\nrate, and the buffer is kept full. When network congestion\noccurs, the client may receive data at a rate lower than the\nobject encoding rate, and the buffer is drained off. If the\nnetwork bandwidth increases before the buffer drops below\nits lower bound, the client will request data at a higher rate\nto fill the buffer. Otherwise, the client will switch to a lower\nrate stream. The selection of the lower rate stream should be\nbased on the following: in a typical bandwidth fluctuation\nperiod, the current bandwidth should be able to maintain\nnormal playback of this lower rate stream and transmit ex-\ntra data to fill the buffer to its upper bound. When the\nnetwork bandwidth is increased, the client may switch to a\nhigher encoding rate stream.\nWe conducted an ideal experiment as a proof of concept of\nthis scheme. We set the lower bound of the buffer size as 5\nseconds to cover the normal stream switch latency as well as\nthe initial buffering duration, as the default play-out buffer\nsize is 5 seconds, and the average stream switch latency is\nalso about 5 seconds. The upper bound of the buffer size is\nset to 30 seconds, considering the typical network fluctua-\ntion periods that may affect streaming quality, such as low\nquality duration, thinning duration, and thinning interval\n(Figures 14(b), 15(a), and 15(b)). In a practical system, the\nlower and upper bound of buffer size should be adaptively\ntunable based on these quality degradation events. However,\nwe will show that even with the above simple configuration,\nthe streaming quality can be effectively improved and the\nover-supplied traffic can be significantly reduced.\nWe simulated the Coordinated Streaming scheme based\non the packet level information of Fast Cache supported\nstreaming sessions, and compared the quality and band-\nwidth usage of this scheme with that of Fast Cache sup-\nported streaming and normal TCP-based streaming. To\nhave a fair comparison, we only consider video sessions that\nrequest objects with 200\u2013400 Kbps encoding rates for a du-\nration longer than 30 seconds in the home user workload.\nFigure 18(a) shows the rebuffering ratio in Fast Cache sup-\nported streaming, normal TCP streaming, and Coordinated\nStreaming. As shown in this figure, the rebuffering ratio of\n"],"Page":11,"Number":14,"Type":"Figure","CaptionBB":[165,541,331,554],"Height":1100,"Width":850,"DPI":100,"ImageBB":[74,394,425,534]}