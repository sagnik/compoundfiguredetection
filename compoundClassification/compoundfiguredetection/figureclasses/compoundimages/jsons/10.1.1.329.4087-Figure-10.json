{"Caption":"Figure 10. Quantitative Evaluation: Comparison of OD-GPDM with and without contextual information on Subject 1. ","ImageText":[{"Text":"(a)","TextBB":[482.125,444.662,494.412,454.802],"Rotation":0},{"Text":"Step","TextBB":[497.18,444.662,516.861,454.802],"Rotation":0},{"Text":"Stool","TextBB":[519.64,444.662,543.019,454.802],"Rotation":0},{"Text":"(b)","TextBB":[653.375,444.662,666.282,454.802],"Rotation":0},{"Text":"Chair","TextBB":[669.05,444.662,693.646,454.802],"Rotation":0}],"Mention":["We trained our context driven model for the sitting ac-\ntivity. As shown in the example of Figure 3, there are sys-\ntematic variations in trajectories in joint-angles and latent\nspace for different heights of the sitting surfaces. The train-\ning dataset for sitting was taken from the CMU-Mocap data\nand included instances with four different seat heights. Fig-\nure 8 shows the latent space after training our model. The\nfour trajectories, shown by different colored points, corre-\nspond to four different instances of sitting.\nFor testing, videos were obtained of subjects sitting on\nchair, stepstool and the ground. Figure 9 shows the perfor-\nmance of context driven OD-GPDM for subject 1. Ground\ntruth was manually hand-labeled to compare the perfor-\nmance of OD-GPDM with and without using contextual\ninformation(Figure 10). It can be seen that use of contex-\ntual information improves the performance of the algorithm.\n"],"Page":7,"Number":10,"Type":"Figure","CaptionBB":[427,482,757,510],"Height":1100,"Width":850,"DPI":100,"ImageBB":[432,288,754,457]}